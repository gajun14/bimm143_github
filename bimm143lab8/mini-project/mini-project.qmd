---
title: "mini-project"
author: "Gaeun Jun - A16814573"
format: pdf
---
### 1. Exploratory data analysis

```{r}
# Save your input data file into your Project directory
fna.data <- "https://bioboot.github.io/bimm143_S20/class-material/WisconsinCancer.csv"
```

```{r}
# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
```

```{r}
head(wisc.df)
```

Creating an unsupervised data set that doesn't contain the diagnosis.

```{r}
# Using -1 to remove the first column of diagnosis
wisc.data <- wisc.df[,-1]
```

Creating a diagnosis vector that contains the diagnosis column. 

```{r}
diagnosis <- wisc.df[,1]
diagnosis <- as.factor(diagnosis)
```

> Q1. How many observations are in this dataset?

```{r}
nrow(wisc.data)
```

There are 569 observations in the wisc.data dataset. 

> Q2. How many of the observations have a malignant diagnosis?

```{r}
table(diagnosis)
```

212 of the observations have a malignant diagnosis. 

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
column_indices <- grep("_mean", colnames(wisc.data))
which_column <- colnames(wisc.data)[column_indices]
which_column
```

```{r}
length(which_column)
```

There are 10 variables/features in the data that are suffixed with "_mean".

### 2. Principal Component Analysis

##### Performing PCA

Checking the mean and standard deviations of the wisc.data columns. 

```{r}
# Check column means and standard deviations
colMeans(wisc.data)

apply(wisc.data,2,sd)
```

```{r}
# Perform PCA on wisc.data
wisc.data <- scale(wisc.data)
wisc.pr <- prcomp(wisc.data)
```

```{r}
# Look at summary of results
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

0.4427 is captured by PC1. 


> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

0.4427 + 0.1897 + 0.09393 = 0.726. 3 principal components are required to describe at least 70% of the original variance in the data. 

> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

0.4427 + 0.1897 + 0.09393 + 0.06602 + 0.05496 + 0.04025 + 0.02251 = 0.91007. 7 principal components are required to describe at least 90% of the original variance in the data. 


##### Interpreting PCA results

```{r}
biplot(wisc.pr)
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

Something that stands out to me about this plot is how much clutter there is, making it impossible to read any of the features. It is difficult to understand because there are so many elements in the plot and overlaps that it is not rational to try to comprehend the plot.  

Generating a plot that's easier to see. 

```{r}
# Scatter plot observations by components 1 and 2
plot(wisc.pr$x[,1], wisc.pr$x[,2], col = diagnosis, 
     xlab = "PC1", ylab = "PC2")
```

> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis,
     xlab = "PC1", ylab = "PC3")
```

Compared to the plot for principal components 1 and 2, this plot seems to have shifted down and slightly more cluttered. This is because PC2 describes more variance in the original data. 

##### Using the ggplot2 package

```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

library(ggplot2)

ggplot(df) +
  aes(PC1, PC2, col=diagnosis) +
  geom_point()
```

##### Variance explained

```{r}
# Calculating the variance of each principal component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)
  
# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

CRAN packages

```{r}
## ggplot based graph
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

### Communicating PCA results

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean", 1]
```

-0.261

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

Using the scree plot, I can calculate how many PC add up to explain at least 80% of the variance of the data: 44.3 + 19 + 9.4 + 6.6 + 5.5 = 84.8. You need at least 5 principal components.

### Hierarchial clustering


```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
```

```{r}
# Calculate the distances between all pairs of observations
data.dist <- dist(data.scaled)
```

```{r}
# Create a hierarchical clustering model using complete linkage
wisc.hclust <- hclust(data.dist, method = "complete")
```

##### Results of hierarchial clustering

> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?


```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

At height = 19, the clustering model has 4 clusters

##### Selecting number of clusters

Using `cutree()` to cut the tree so that it has 4 clusters. 

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

```{r}
wisc.hclust.clusters_test <- cutree(wisc.hclust, k=5)
table(wisc.hclust.clusters_test, diagnosis)
```


> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?

When I cut the amount of clusters to 5 clusters I get a better match because cluster 2 shows a clearer distinction between malignant and benign cases (while the other clusters remain the same) while cluster 2 in the 4 clusters show less of a variance. 

##### Using different methods

> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

```{r}
methods <- c("single", "complete", "average", "ward.D2")

results <- list()

# Loop through the methods and store the clusters
for (method in methods) {
  # Create the hierarchical clustering model
  hclust_model <- hclust(data.dist, method = method)
  
  clusters <- cutree(hclust_model, k = 4)
  
  results[[method]] <- table(clusters, diagnosis)
}

results
```

Out of these methods, the "complete" and "ward.D2" methods give me the best results because they show the clearest distinction between the benign and malignant cases. However, it's hard to compare these two because while one might have a bigger discrepancy between the two, the other has a clearer indication that a certain cluster corresponds to either malignant or benign cells. 

### OPTIONAL: K-means clustering

##### K-means clustering and comparing results

Creating a k-means model on wisc.data

```{r}

scaled_data <- scale(wisc.data)
wisc.km <- kmeans(scaled_data, centers= 2, nstart= 20)
```

```{r}
table(wisc.km$cluster, diagnosis)
```

> Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

```{r}
wisc.hclust.clusters.2 <- cutree(wisc.hclust, k=2)
table(wisc.hclust.clusters.2, diagnosis)
```
Compared to the hclust results, k-means definitely separates the two diagnoses a lot better, because the first cluster clearly shows an indication towards benign cells and cluster 2 showing an indication towards malignant cells. In the hclust results, when I use 2 clusters, the first cluster is pretty divided between the two cells. 

```{r}
table(wisc.hclust.clusters, wisc.km$cluster)
```

### 5. Combining methods

##### Clustering on PCA results

```{r}
pca_result <- prcomp(scaled_data, center = TRUE, scale. = TRUE)

pve <- summary(pca_result)$importance[3, ]
components_needed <- which(pve >= 0.90)[1]

pca_data <- pca_result$x[, 1:components_needed]

wisc.pr.hclust <- hclust(dist(pca_data), method = "ward.D2")

plot(wisc.pr.hclust)
```

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```


```{r}
plot(wisc.pr$x[,1:2], col=grps)
```

```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```

Swapping the colors of the clusters, such that cluster 2 comes first

```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```

```{r}
## Use the distance along the first 7 PCs for clustering
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
```


```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
```

```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```

> Q15. How well does the newly created model with four clusters separate out the two diagnoses?

It separates out the four clusters as the same as k-means. It separates it out pretty well as the first cluster indicates malignant cells and the second cluster indicates benign cells. 

> Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

```{r}
table(wisc.km$cluster, diagnosis)
```

```{r}
table(wisc.hclust.clusters, diagnosis)
```

Both k-means and hierarchial clustering can separate the diagnosis well, because each cluster tends to lean towards a diagnosis with a good proportion. Even ward.d2 can separate the diagnosis well. 

### 6. Sensitivity/Specificity

Sensitivity: cluster with predominantly malignant cells / total number of known malignant samples

```{r}
sum(diagnosis == "M")
```

The different methods/procedures for sensitivity: 

hcluster: 165/212 = 0.778

k-means: 175/212 = 0.825

K-means results in a clustering model with the best sensitivity. 

Specificity: cluster with predominantly benign cells / total number of known benign samples

```{r}
sum(diagnosis == "B")
```

hcluster: 343/357 = .961

k-means: 343/357 = .961

Both hcluster and k-means result in a clustering model with the best specificity. 

### 7. Prediction

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```

> Q18. Which of these new patients should we prioritize for follow up based on your results?

Patient 2 should prioritize for follow-up because they have more malignant cells while patient 1 has benign cells. 
